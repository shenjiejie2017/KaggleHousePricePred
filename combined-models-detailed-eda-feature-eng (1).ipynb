{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-24T23:15:46.454193Z","iopub.execute_input":"2022-04-24T23:15:46.454603Z","iopub.status.idle":"2022-04-24T23:15:46.464796Z","shell.execute_reply.started":"2022-04-24T23:15:46.45457Z","shell.execute_reply":"2022-04-24T23:15:46.463799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# House Price Prediction\n\nThis project will try to predict housing price and see if there are some unexpected features that could predict housing price well in this case.\n\nSome coding credit belong to :\nhttps://github.com/krishnaik06/Advanced-House-Price-Prediction-\nhttps://www.kaggle.com/code/apapiu/regularized-linear-models\nhttps://www.kaggle.com/code/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition","metadata":{}},{"cell_type":"markdown","source":"## Part 0: Load data and packages","metadata":{}},{"cell_type":"code","source":"#Load packages \n\n!pip install pydotplus\nfrom IPython.display import Image  \nfrom sklearn import tree\nimport pydotplus\nimport pandas as pd\nimport numpy as np\nimport collections\nfrom math import sqrt\nimport scipy.stats as ss\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\n\nfrom sklearn import preprocessing, tree\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, MinMaxScaler,StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, LassoCV, LassoLarsCV, Ridge, RidgeCV \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nimport xgboost\n\n#Special display options\npd.set_option(\"display.max_columns\", None) ","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:36:18.512585Z","iopub.execute_input":"2022-04-25T04:36:18.513007Z","iopub.status.idle":"2022-04-25T04:36:27.002863Z","shell.execute_reply.started":"2022-04-25T04:36:18.512973Z","shell.execute_reply":"2022-04-25T04:36:27.001454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load data\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:36:27.081364Z","iopub.execute_input":"2022-04-25T04:36:27.081817Z","iopub.status.idle":"2022-04-25T04:36:27.149391Z","shell.execute_reply.started":"2022-04-25T04:36:27.08177Z","shell.execute_reply":"2022-04-25T04:36:27.148011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I would like to distinguish year, numerical, and categorical features first so I can run them and skip some EDA steps to later feature engineering\n#But it's after some EDA I know how to distinguish year features\n\n#Find year features\ncol_year= [feature for feature in train.columns if \"Year\" in feature or \"Yr\" in feature]\ncol_year_test = [feature for feature in test.columns if \"Year\" in feature or \"Yr\" in feature]\n\n#Find numerical & cateogorical features for training data\ncol_NonYr= [feature for feature in train.columns if feature not in col_year]\ncol_num=[]\ncol_cat=[]\nfor c in col_NonYr:    \n    if (train[c].dtype!='O') and (c not in col_year):\n        col_num.append(c)\n    else:\n        col_cat.append(c)\n\n#Find numerical & cateogorical features for testing data\ncol_NonYr_test= [feature for feature in test.columns if feature not in col_year_test]\ncol_num_test=[]\ncol_cat_test=[]\nfor c in col_NonYr_test:    \n    if (test[c].dtype!='O') and (c not in col_year_test):\n        col_num_test.append(c)\n    else:\n        col_cat_test.append(c)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:36:29.999098Z","iopub.execute_input":"2022-04-25T04:36:29.999555Z","iopub.status.idle":"2022-04-25T04:36:30.018226Z","shell.execute_reply.started":"2022-04-25T04:36:29.999518Z","shell.execute_reply":"2022-04-25T04:36:30.016907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 1: EDA (Exploratory Data Analytics)\n","metadata":{}},{"cell_type":"markdown","source":"### Light EDA of overall data","metadata":{}},{"cell_type":"code","source":"#Find out number of rows & columns in data\nprint ('Training set Number of (rows,columns): ' + (str(train.shape) ))\nprint ('Testing set Number of (rows,columns): ' + str(test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:54.432946Z","iopub.execute_input":"2022-04-24T23:15:54.433637Z","iopub.status.idle":"2022-04-24T23:15:54.451153Z","shell.execute_reply.started":"2022-04-24T23:15:54.433587Z","shell.execute_reply":"2022-04-24T23:15:54.449736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check some statistics properties of training data for numerical features\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:54.452731Z","iopub.execute_input":"2022-04-24T23:15:54.45334Z","iopub.status.idle":"2022-04-24T23:15:54.587072Z","shell.execute_reply.started":"2022-04-24T23:15:54.453304Z","shell.execute_reply":"2022-04-24T23:15:54.585934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: Seems there are lots of columns having median values of 0 while the mean aren't. Those columns probably have lots of zero values.","metadata":{}},{"cell_type":"markdown","source":"### Check dependent variable sale price \nFirst we will check the distribution of sale price. Histogram, skewness, kurtosis are good descriptive statistic tools to use. And since for linear regression model, the dependent variable should be normally distributed, I used probability plot too. \n","metadata":{}},{"cell_type":"code","source":"#Check skewness & kurtosis of dependent variable SalePrice\nprint(\"Training data \\\"SalePrice\\\" skewness: %f\" % train['SalePrice'].skew())\nprint(\"Training data \\\"SalePrice\\\" kurtosis: %f\" % train[\"SalePrice\"].kurt())\n\n#Check distribution of dependent variable SalePrice \nplt.figure(figsize=(6,3))\nsns.distplot(train['SalePrice'], fit= norm)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:54.588473Z","iopub.execute_input":"2022-04-24T23:15:54.588794Z","iopub.status.idle":"2022-04-24T23:15:54.888222Z","shell.execute_reply.started":"2022-04-24T23:15:54.588762Z","shell.execute_reply":"2022-04-24T23:15:54.886851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use probability plot to check normality of data\nplt.figure(figsize=(6,3))\nres=stats.probplot(train['SalePrice'], plot=plt)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:54.889538Z","iopub.execute_input":"2022-04-24T23:15:54.889869Z","iopub.status.idle":"2022-04-24T23:15:55.065069Z","shell.execute_reply.started":"2022-04-24T23:15:54.889838Z","shell.execute_reply":"2022-04-24T23:15:55.063555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Boxplot is a good way to check distribution, spread, and outlier of numerical variables \nplt.figure(figsize=(6,3))\ntrain.boxplot(column =['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:55.069278Z","iopub.execute_input":"2022-04-24T23:15:55.0697Z","iopub.status.idle":"2022-04-24T23:15:55.238792Z","shell.execute_reply.started":"2022-04-24T23:15:55.069665Z","shell.execute_reply":"2022-04-24T23:15:55.237528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: Seems sale price is extreamly right skewed (skewness >1)  and has a postive kurtosis (kurtosis >3). The sale price isn't normally ditributed and there are lots of outliers. Since linear regression has normality assumption on both dependent and independent variables and are very sensitive to outliers, we can't just use this to train our model. Log transformation is usually one good way to fix this. Let't try and see if things improve.","metadata":{}},{"cell_type":"code","source":"#Perform some log normalization and see how sale price distribution improved\n#Since we are only on EDA, I only perform transformation on copy of data\ndata = train.copy()\ndata['SalePrice'] = np.log(data[\"SalePrice\"])\nprint(\"Training data \\\"SalePrice\\\" skewness: %f\" % data['SalePrice'].skew())\nprint(\"Training data \\\"SalePrice\\\" kurtosis: %f\" % data[\"SalePrice\"].kurt())\nplt.figure(figsize=(6,3))\nsns.distplot(data['SalePrice'], fit= norm)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:55.240899Z","iopub.execute_input":"2022-04-24T23:15:55.241269Z","iopub.status.idle":"2022-04-24T23:15:55.543011Z","shell.execute_reply.started":"2022-04-24T23:15:55.241236Z","shell.execute_reply":"2022-04-24T23:15:55.54169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use probability plot again to check normality of data\nplt.figure(figsize=(6,3))\nres=stats.probplot(data['SalePrice'], plot=plt)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:55.544435Z","iopub.execute_input":"2022-04-24T23:15:55.544868Z","iopub.status.idle":"2022-04-24T23:15:55.735696Z","shell.execute_reply.started":"2022-04-24T23:15:55.544831Z","shell.execute_reply":"2022-04-24T23:15:55.734512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now check outliers again using boxplot\nplt.figure(figsize=(6,3))\ndata.boxplot(column =['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:55.736993Z","iopub.execute_input":"2022-04-24T23:15:55.737274Z","iopub.status.idle":"2022-04-24T23:15:55.895016Z","shell.execute_reply.started":"2022-04-24T23:15:55.737247Z","shell.execute_reply":"2022-04-24T23:15:55.893797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: After log transformation, the sale price is much more normally distributed and the kutosis changed from positive (kurtosies>3) to negative (kurtosis<3). The skewness changed from extreamly right skewed (skewness >1) to almost symmetrical (-0.5 < skewness < 0.5). There are less high sale price outliers. I am satisfied with the transformation results. Later when we train the model, we will use the log transfomation on sale price.","metadata":{}},{"cell_type":"markdown","source":"### Missing values and duplicated values","metadata":{}},{"cell_type":"code","source":"# Check missing values and duplicated values \nprint ('Missing value:', train.isnull().sum().sum())\nprint ('Duplicated rows:', train.duplicated().sum())\nprint ('Duplicated columns:',train.columns.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:55.897418Z","iopub.execute_input":"2022-04-24T23:15:55.897856Z","iopub.status.idle":"2022-04-24T23:15:55.93827Z","shell.execute_reply.started":"2022-04-24T23:15:55.897811Z","shell.execute_reply":"2022-04-24T23:15:55.937464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check which columns have missing values in training and testing data\ncol_train_with_missing=[c for c in train.columns if train[c].isnull().sum()>=1]\ncol_test_with_missing=[c for c in test.columns if test[c].isnull().sum()>=1]\n\n#Check if missing value columns are consistent in training vs. testing\nprint(f\"Have missing values in testing but not in training data: { set(col_test_with_missing).difference(set(col_train_with_missing)) }\\n\")\nprint(f\"Have missing values in training but not in testing data: { set(col_train_with_missing).difference(set(col_test_with_missing)) }\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:55.939262Z","iopub.execute_input":"2022-04-24T23:15:55.939627Z","iopub.status.idle":"2022-04-24T23:15:56.007594Z","shell.execute_reply.started":"2022-04-24T23:15:55.939595Z","shell.execute_reply":"2022-04-24T23:15:56.006236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"code","source":"#Plot to see % of missing values in each columns \nplt.figure(figsize=(6,3))\ntrain[col_train_with_missing].isnull().mean().sort_values(ascending=False).plot.bar(ylabel=\"Missing value %\", title=\"Missing value% of columns with missing values\",color='cadetblue')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:56.009127Z","iopub.execute_input":"2022-04-24T23:15:56.009616Z","iopub.status.idle":"2022-04-24T23:15:56.372003Z","shell.execute_reply.started":"2022-04-24T23:15:56.009578Z","shell.execute_reply":"2022-04-24T23:15:56.370529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take a look to see records with missing values\ntrain[train[col_train_with_missing].isnull().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:56.374741Z","iopub.execute_input":"2022-04-24T23:15:56.375246Z","iopub.status.idle":"2022-04-24T23:15:56.48787Z","shell.execute_reply.started":"2022-04-24T23:15:56.375198Z","shell.execute_reply":"2022-04-24T23:15:56.486459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: \nThere are columns with lots of missing values. From the output above, it seems if the value isn't relavant, it will be missing (e.g., if there is 0 \"FirePlaces\", \"FireplaceQu\" would be \"NaN\"). So, for each numerical features, I'll fill in missing vaues and also have other columns to indicate where missing values are filled. For categorical features, filling them with \"missing\" is enough to servce both purposes. I'll do this in later feature preprocessing.\n\nAlso, it seems testing data has similar columns with missing values but just a few more. So, we can deal with them similarly.","metadata":{}},{"cell_type":"markdown","source":"### Datetime variables\n","metadata":{}},{"cell_type":"code","source":"#Check columns types of training data. \ntrain[col_year].info()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:56.489879Z","iopub.execute_input":"2022-04-24T23:15:56.490264Z","iopub.status.idle":"2022-04-24T23:15:56.507124Z","shell.execute_reply.started":"2022-04-24T23:15:56.490223Z","shell.execute_reply":"2022-04-24T23:15:56.505643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use plots to see relationship of target variable SalePrice and Year features.\nfig=plt.figure(figsize=(30,5))\nfor count, feature in enumerate(col_year, 1):\n    data= train.copy()\n    ax=fig.add_subplot(1,len(col_year),count)\n    if feature == 'YrSold':\n        ax.plot(data.groupby(feature)[\"SalePrice\"].median(), color='fuchsia')\n        ax.annotate(\"Price drops with yr sold makes no sense!\", xy=[2008.0,164000], xytext=[2006.0, 160000], arrowprops={'arrowstyle':\"->\",'color':\"blue\"})\n    else:\n        ax.plot(data.groupby(feature)[\"SalePrice\"].median(), color=\"slategray\")\n    ax.set_xlabel(feature, fontsize='xx-large')\n    ax.set_ylabel(\"SalePrice ($)\")\n    \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:56.509156Z","iopub.execute_input":"2022-04-24T23:15:56.509674Z","iopub.status.idle":"2022-04-24T23:15:57.172303Z","shell.execute_reply.started":"2022-04-24T23:15:56.509631Z","shell.execute_reply":"2022-04-24T23:15:57.170554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: It seems \"YrSold\" isn't a good feature to use directly since the sale price drop as the year increase. But the difference between year sold and other year features would be some ages of the houses, which might be good features to predict sale price. Let's draw some plots and see how it goes.","metadata":{}},{"cell_type":"code","source":"#boxplot to take a look at year distribution\nsns.boxplot(data=train[col_year])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:57.174731Z","iopub.execute_input":"2022-04-24T23:15:57.175263Z","iopub.status.idle":"2022-04-24T23:15:57.404344Z","shell.execute_reply.started":"2022-04-24T23:15:57.175213Z","shell.execute_reply":"2022-04-24T23:15:57.402914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a few features based on \"YrSold\", which are difference of year features with year sold and draw plots to see relationships again\n\nfig=plt.figure(figsize=(30,5))\nfor count, feature in enumerate(['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'], 1):\n    data= train.copy()\n    data[feature]=data['YrSold'] - data[feature]\n    ax=fig.add_subplot(1,len(col_year),count)\n    ax.plot(data.groupby(feature)[\"SalePrice\"].median(), color=\"slategray\")\n    ax.set_xlabel(\"YearSold- \" +str(feature), fontsize='xx-large' )\n    ax.set_ylabel(\"SalePrice ($)\")\nplt.show ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:57.405986Z","iopub.execute_input":"2022-04-24T23:15:57.406322Z","iopub.status.idle":"2022-04-24T23:15:57.972907Z","shell.execute_reply.started":"2022-04-24T23:15:57.40629Z","shell.execute_reply":"2022-04-24T23:15:57.971444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: Now the relationship seems reasonable. We will use those new features in later feature preprocessing.","metadata":{}},{"cell_type":"markdown","source":"## Numerical variables\n\nI will first try to see if I can use linear regression model. There are 4 basic assumptions of linear regression, I should check and see if I can transform data accordingly to fit those assumptions in order for our model to perform well: <br>\n1. Linearity <br>\n2. Homoscedasticity <br>\n3. Independence <br>\n4. Normality <br>\n\n#### Other important things for linear regression model  \n1. Feature scaling - linear regression uses gradient descent to find optimal fitting line so scaling is required <br>\n2. Impact of missing values - linear regression is sensitive to missing values so need to be careful to handle them <br>\n3. Impact of outliers -  linear regression needs the relationship between the independent variable and dependent variable to be linear. So, outliers have big impact on model performance although regularization will help.<br>\n\n","metadata":{}},{"cell_type":"code","source":"#Check counts of each data type for training and testing data\nprint(\"There are {} numerical fields, {} categorical fields in training data\".format(len(col_num),len(col_cat)))\nprint(\"There are {} numerical fields, {} categorical fields in testing data\".format(len(col_num_test),len(col_cat_test)))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:57.974903Z","iopub.execute_input":"2022-04-24T23:15:57.975244Z","iopub.status.idle":"2022-04-24T23:15:57.983341Z","shell.execute_reply.started":"2022-04-24T23:15:57.975213Z","shell.execute_reply":"2022-04-24T23:15:57.981738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discrete and continuous numerical variables\n#### I will treat discrete variables more like categorical variables so I need to distinguish them first","metadata":{}},{"cell_type":"code","source":"#Check unique values in each numerical feature (training data)\nuniques={}\nfor feature in col_num:\n    unique=len(train[feature].unique())\n    uniques[feature]=unique    \n\n#Sorted the unique values\nuniques_sorted = {k: v for k, v in sorted(uniques.items(), key=lambda item: item[1])}\nfor pair in uniques_sorted.items():\n  print(pair)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:57.984811Z","iopub.execute_input":"2022-04-24T23:15:57.985104Z","iopub.status.idle":"2022-04-24T23:15:58.004451Z","shell.execute_reply.started":"2022-04-24T23:15:57.985076Z","shell.execute_reply":"2022-04-24T23:15:58.003487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: Seems in this case, there is a jump from unique values count 24 to 76. I will set the cut-off point to be 25. When there are less than 25 unique values in a numerical feature, I will define it as a discrete feature.","metadata":{}},{"cell_type":"code","source":"#Seperate numerical features by discrete and continuous features in trainng and testing data\ncol_disc=[feature for feature in col_num if len(data[feature].unique())<=25]\ncol_cont=[feature for feature in col_num if feature not in col_disc and feature != \"Id\"]\n\ncol_disc_test=[feature for feature in col_num_test if len(test[feature].unique())<=25]\ncol_cont_test=[feature for feature in col_num_test if feature not in col_disc_test]\n\nprint(f\"Discrete features: {col_disc}\\n\")\nprint(f\"Continuous features: {col_cont}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:58.005899Z","iopub.execute_input":"2022-04-24T23:15:58.006908Z","iopub.status.idle":"2022-04-24T23:15:58.022411Z","shell.execute_reply.started":"2022-04-24T23:15:58.006844Z","shell.execute_reply":"2022-04-24T23:15:58.021555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear regression assumption check - normality","metadata":{}},{"cell_type":"code","source":"#Draw histograms for all continuous numerical features (training data)\ngrh_per_row =3\nfig, ax = plt.subplots(len(col_cont)//grh_per_row+1,grh_per_row, figsize = (30, 30))\n\nfor count, feature in enumerate(col_cont, 0):\n       data= train.copy()\n       row =count // grh_per_row\n       col=(count )% grh_per_row    \n       ax[row,col].hist( train[feature], color=\"thistle\")\n       ax[row,col].set_xlabel(feature, fontsize='xx-large') \n       ax[row,col].set_ylabel(\"Count\") \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:15:58.027213Z","iopub.execute_input":"2022-04-24T23:15:58.027922Z","iopub.status.idle":"2022-04-24T23:16:00.790224Z","shell.execute_reply.started":"2022-04-24T23:15:58.027882Z","shell.execute_reply":"2022-04-24T23:16:00.78902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: Seems most of the numerical fields aren't normally distributed.","metadata":{}},{"cell_type":"markdown","source":"### Linear regression assumption check - linearity, homoscedasticity","metadata":{}},{"cell_type":"code","source":"#Draw scatter plots for all continuous numerical features (training data) \n#Scatter plot is good way to see linearity, homoscedasticity, and even some normality\ngrh_per_row=3\nfig, ax = plt.subplots(len(col_cont)//grh_per_row,grh_per_row, figsize = (40,40))\n\nfor count, feature in enumerate(col_cont, 0):\n    if feature==\"SalePrice\":\n        pass\n    else:\n      data= train.copy()\n      row=count // grh_per_row\n      col=(count )% grh_per_row\n      ax[row,col].scatter(data[feature],data['SalePrice'], color=\"tan\")    \n      ax[row,col].set_xlabel(feature, fontsize='xx-large')\n      ax[row,col].set_ylabel(\"SalePrice ($)\")    \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:16:00.793172Z","iopub.execute_input":"2022-04-24T23:16:00.793533Z","iopub.status.idle":"2022-04-24T23:16:03.308689Z","shell.execute_reply.started":"2022-04-24T23:16:00.793502Z","shell.execute_reply":"2022-04-24T23:16:03.307557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: \nIt seems most features violate the following: <br>\n1. Homoscedasticity. The variance of residual isn't the same for all values of x <br>\n2. Normality \n\nOne way to fix this is using log transofmration on both dependent and independent variables. This method would probably fix both homoscedasticity and normality a bit. We will try this in later feature engineering.","metadata":{}},{"cell_type":"code","source":"#Draw histograms for discrete features (training data)\ngrh_per_row =3\nfig, ax = plt.subplots(len(col_disc)//grh_per_row+1,grh_per_row, figsize = (30, 30))\n\nfor count, feature in enumerate(col_disc, 0):\n    data= train.copy()\n    row=count // grh_per_row\n    col=(count )% grh_per_row    \n    ax[row,col].hist(train[feature],color=\"lightsteelblue\", bins=data[feature].unique().sort()) \n    ax[row,col].set_xlabel(feature, fontsize='xx-large')   \n    ax[row,col].set_ylabel(\"Count\") \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:16:03.310206Z","iopub.execute_input":"2022-04-24T23:16:03.31083Z","iopub.status.idle":"2022-04-24T23:16:06.380643Z","shell.execute_reply.started":"2022-04-24T23:16:03.310778Z","shell.execute_reply":"2022-04-24T23:16:06.379471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Draw scatter plots of all discrete numerical features (training data) \ngrh_per_row=3\nfig, ax = plt.subplots(len(col_disc)//grh_per_row+1,grh_per_row, figsize = (30,30))\n\nfor count, feature in enumerate(col_disc, 0):\n    data= train.copy()\n    row=count // grh_per_row\n    col=(count )% grh_per_row\n    ax[row,col].scatter(data[feature],data['SalePrice'], color=\"cornflowerblue\")    \n    ax[row,col].set_xlabel(feature, fontsize='xx-large')\n    ax[row,col].set_ylabel(\"SalePrice ($)\")    \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:16:06.382423Z","iopub.execute_input":"2022-04-24T23:16:06.383065Z","iopub.status.idle":"2022-04-24T23:16:10.826213Z","shell.execute_reply.started":"2022-04-24T23:16:06.383006Z","shell.execute_reply":"2022-04-24T23:16:10.825177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: We can see housing price can be different for some categories in some discrete numerical features. For example, price seem to be higher for higher \"OverallCond\" and \"OverallQual\". Those would be good predictive features to use to train models.","metadata":{}},{"cell_type":"markdown","source":"### Linear regression assumption check - Independence\nLinear regression require features to be independent to each other. Since we have so many features, it's likely lots of them are dependent to each other. Let's take a look.","metadata":{}},{"cell_type":"code","source":"# Use heatmap to check feature correlations\ncorr=train[col_num].corr()\nplt.figure(figsize=(25,23))\nplt.title(\"Housing data numerical feature correlation\")\nsns.heatmap(data=corr, annot=True, cmap=\"BuPu\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:16:10.827675Z","iopub.execute_input":"2022-04-24T23:16:10.828219Z","iopub.status.idle":"2022-04-24T23:16:17.420212Z","shell.execute_reply.started":"2022-04-24T23:16:10.828164Z","shell.execute_reply":"2022-04-24T23:16:17.419249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: According to this article (https://www.dummies.com/article/academics-the-arts/math/statistics/how-to-interpret-a-correlation-coefficient-r-169792), here is how I would interprete correlations:\n\n- no linear relationship: [-0.3, 0.3]\n- weak/none correlation: [-0.3, -0.5] or [0.3, 0.5]\n- moderate: [-0.5, -0.7] or [0.5, 0.7]\n- strong: <=-0.7 or >=0.7\n\nIt seems more than 50% features have moderate correlations with a few features. A few has strong correlations with each other. For example, \"1stFlrSF\" are highly correlated to \"TotalBsmtSF\". We might just keep one of them since they are probably very similar. For 2 features which are highly correlated to each other, Lasso regression would probably drop one by setting the coefficient to 0 and keep the other one. We would need to do regularization later in model training.","metadata":{}},{"cell_type":"markdown","source":"## Categorical variables","metadata":{}},{"cell_type":"code","source":"#Check to see categorical features' relationships with sale price\ngrh_per_row=3\nfig, ax = plt.subplots(len(col_cat)//grh_per_row+1,grh_per_row, figsize = (40, 120))\n\nfor count, feature in enumerate(col_cat, 0):\n    data= train.copy()\n    row=count //grh_per_row\n    col=(count )% grh_per_row \n    df = data.groupby(feature)['SalePrice'].mean().to_frame('SalePrice')\n    \n    ax[row,col].bar(df.index, df.SalePrice, color=\"burlywood\")\n    ax[row,col].set_xlabel(feature, fontsize='xx-large')\n    ax[row,col].set_ylabel(\"SalePrice ($)\") \n    ax[row,col].set_xticklabels(df.index, fontsize=\"large\",rotation=30 )  \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:16:17.421532Z","iopub.execute_input":"2022-04-24T23:16:17.422098Z","iopub.status.idle":"2022-04-24T23:16:24.895087Z","shell.execute_reply.started":"2022-04-24T23:16:17.422059Z","shell.execute_reply":"2022-04-24T23:16:24.894127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: \nThere are some categories in some cateogorical features having obviously higher or lower sale prices. For example, \"SaleCondition\",\"Neighborhood\", and \"CentralAir\". But that depends on the count for each unique values. For example, if there aren't many houses with \"Partial\" sale condition, \"partial\" sale condition may not imply higher sale price. \n\nSome features have so many unique values. Since we already have so many features, we need to be careful not to create too many additional columns when do categorical feature encoding. I will try to put rare categories in each categorical feature together.","metadata":{}},{"cell_type":"code","source":"#Let's check the counts of each categorical features to find the cut-off count for rare features\nfor feature in col_cat:\n    data=train.copy()\n    print(feature)\n    print(data[feature].value_counts())\n    print(\"______________________\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:37:08.242575Z","iopub.execute_input":"2022-04-25T04:37:08.243174Z","iopub.status.idle":"2022-04-25T04:37:08.400848Z","shell.execute_reply.started":"2022-04-25T04:37:08.243134Z","shell.execute_reply":"2022-04-25T04:37:08.399311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: Seems category count < 15 is a good cut-off point. I will later group those with <1% (count < 15) features as rare features in feature engineering.","metadata":{}},{"cell_type":"markdown","source":"## Part 2: Feature preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Missing values - Numerical variables","metadata":{}},{"cell_type":"code","source":"#As seen in the data EDA, missing values have meanings. I replace missing values with median\nfor feature in col_num:\n    if train[feature].isnull().sum()>0:\n      train[feature+\"_nan\"]=np.where(train[feature].isnull(),1,0)\n      train[feature].fillna(train[feature].median(),inplace=True)\n\nfor feature in col_num_test:\n    if test[feature].isnull().sum()>0:\n      test[feature+\"_nan\"]=np.where(test[feature].isnull(),1,0)\n      test[feature].fillna(test[feature].median(), inplace=True)    \n    \nprint(train[col_num].isnull().sum().sum())\nprint(test[col_num_test].isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:44:26.928115Z","iopub.execute_input":"2022-04-25T04:44:26.928776Z","iopub.status.idle":"2022-04-25T04:44:26.982965Z","shell.execute_reply.started":"2022-04-25T04:44:26.928735Z","shell.execute_reply":"2022-04-25T04:44:26.981674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing values - Categorical variables","metadata":{}},{"cell_type":"code","source":"#Fill in categorical missing values with \"Missing\"\nfor feature in col_cat:\n      train[feature].fillna(\"Missing\",inplace=True)\n\nfor feature in col_cat_test:\n      test[feature].fillna(\"Missing\", inplace=True)    \n\n#Check missing values again    \nprint(train[col_cat].isnull().sum().sum())\nprint(test[col_cat_test].isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:44:29.642516Z","iopub.execute_input":"2022-04-25T04:44:29.642913Z","iopub.status.idle":"2022-04-25T04:44:29.717979Z","shell.execute_reply.started":"2022-04-25T04:44:29.64288Z","shell.execute_reply":"2022-04-25T04:44:29.716538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing values - Year variables ","metadata":{}},{"cell_type":"code","source":"#Fill in missing values for year variables similar to numerical ones\nfor feature in col_year:\n    if train[feature].isnull().sum()>0:\n       train[feature+\"_nan\"]=np.where(train[feature].isnull(),1,0)\n       train[feature].fillna(train[feature].median(),inplace=True)\n\nfor feature in col_year_test:\n    if test[feature].isnull().sum()>0:\n      test[feature+\"_nan\"]=np.where(test[feature].isnull(),1,0)\n      test[feature].fillna(test[feature].median(), inplace=True)   \n    \n#Check missing values again    \nprint(train.isnull().sum().sum())\nprint(test.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:44:32.139155Z","iopub.execute_input":"2022-04-25T04:44:32.139564Z","iopub.status.idle":"2022-04-25T04:44:32.180528Z","shell.execute_reply.started":"2022-04-25T04:44:32.139531Z","shell.execute_reply":"2022-04-25T04:44:32.179582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 3: Feature engineering","metadata":{}},{"cell_type":"markdown","source":"### Create some needed additional fields","metadata":{}},{"cell_type":"code","source":"#As we've discussed in EDA, we need to create new features based on YrSold\n#There are a few records having other year features > YrSold (i.e.,GarageYrBlt > YrSold). Those are data errors. Gladly they are very few.\n#Since there are only around 2 of records having this problem, I just set those new features to be 0 if they are negative\nfor feature in col_year:\n    if feature !=\"YrSold\": \n       train[\"YrSold-\"+feature]=np.maximum(train[\"YrSold\"] - train[feature],0)\n\nfor feature in col_year_test:\n    if feature !=\"YrSold\": \n      test[\"YrSold-\"+feature]=np.maximum(test[\"YrSold\"] - test[feature],0)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:44:36.123406Z","iopub.execute_input":"2022-04-25T04:44:36.123818Z","iopub.status.idle":"2022-04-25T04:44:36.13861Z","shell.execute_reply.started":"2022-04-25T04:44:36.123784Z","shell.execute_reply":"2022-04-25T04:44:36.13693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Features Encoding\nAs seen in EDA, some categorical features are with very rare categories. The cut off count for them is 15 or 1%. I will group those <=1% categories together for later encoding","metadata":{}},{"cell_type":"code","source":"#Group < 1% rare categories together \nfor feature in col_cat:\n    temp=train.groupby(feature)['SalePrice'].count()/len(train)\n    temp_df=temp[temp>0.01].index\n    train[feature]=np.where(train[feature].isin(temp_df),train[feature],'Rare_var')\n    test[feature]=np.where(test[feature].isin(temp_df),test[feature],'Rare_var')","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:44:42.184521Z","iopub.execute_input":"2022-04-25T04:44:42.184957Z","iopub.status.idle":"2022-04-25T04:44:42.307049Z","shell.execute_reply.started":"2022-04-25T04:44:42.18492Z","shell.execute_reply":"2022-04-25T04:44:42.305739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Label encoding refering to average sale price of each \"label\"\nfor feature in col_cat:\n    labels_ordered=train.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    train[feature]=train[feature].map(labels_ordered)\n    test_default = collections.defaultdict(lambda: 0.0, labels_ordered) #map categories not in training data but in testing to 0\n    test[feature]=test[feature].map( test_default)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:44:44.201649Z","iopub.execute_input":"2022-04-25T04:44:44.202104Z","iopub.status.idle":"2022-04-25T04:44:44.443202Z","shell.execute_reply.started":"2022-04-25T04:44:44.202064Z","shell.execute_reply":"2022-04-25T04:44:44.44198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#log transform the target variable:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#log transform skewed numeric features (note to add 1 since some features are zero):\nskewed_feats = train[col_num].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\ntrain[skewed_feats] = np.log1p(train[skewed_feats])\ntest[skewed_feats] = np.log1p(test[skewed_feats])\n\n#log transform categorical features:\ntrain[col_cat] = np.log1p(train[col_cat])\ntest[col_cat] = np.log1p(test[col_cat])","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:44:48.785232Z","iopub.execute_input":"2022-04-25T04:44:48.786051Z","iopub.status.idle":"2022-04-25T04:44:48.903603Z","shell.execute_reply.started":"2022-04-25T04:44:48.786004Z","shell.execute_reply":"2022-04-25T04:44:48.902549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Draw scatter plots for all features in training data \ngrh_per_row=4\nfig, ax = plt.subplots(len(train.columns)//grh_per_row+1,grh_per_row, figsize = (30, 100))\n\nfor count, feature in enumerate(train.columns, 0):\n    data= train.copy()\n    row=count // grh_per_row\n    col=(count )% grh_per_row\n    ax[row,col].scatter(data[feature],data['SalePrice'], color=\"mediumpurple\")    \n    ax[row,col].set_xlabel(feature, fontsize='xx-large')\n    ax[row,col].set_ylabel(\"SalePrice with log normalization ($)\")   \n    ax[row,col].grid()\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:16:25.69913Z","iopub.execute_input":"2022-04-24T23:16:25.699856Z","iopub.status.idle":"2022-04-24T23:16:41.365039Z","shell.execute_reply.started":"2022-04-24T23:16:25.699806Z","shell.execute_reply":"2022-04-24T23:16:41.363333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove some outliers","metadata":{}},{"cell_type":"code","source":"#As we can see from the plots above, there are some outliers. Let's removing some of them.\n\ntrain = train.drop(train[(train['LotFrontage']>5)].index)\ntrain = train.drop(train[(train['LotArea']>11.5) ].index)\ntrain = train.drop(train[(train['YearBuilt']<1900) & (train['SalePrice']>12.4)].index)\ntrain = train.drop(train[(train['1stFlrSF']>8) & (train['SalePrice']<12.5)].index)\n\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:44:59.687229Z","iopub.execute_input":"2022-04-25T04:44:59.688058Z","iopub.status.idle":"2022-04-25T04:44:59.715833Z","shell.execute_reply.started":"2022-04-25T04:44:59.68801Z","shell.execute_reply":"2022-04-25T04:44:59.714524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 4: Model training","metadata":{}},{"cell_type":"markdown","source":"### Linear Regression with regularizations","metadata":{}},{"cell_type":"code","source":"#Drop \"Id\" and seperate independent variables vs. target variables for training data\ny_train=train['SalePrice']\nX_train=train.drop([\"SalePrice\",\"Id\"], axis=1)\n\n#Drop \"Id\" from testing data\nX_test=test.drop([\"Id\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:45:04.995697Z","iopub.execute_input":"2022-04-25T04:45:04.996438Z","iopub.status.idle":"2022-04-25T04:45:05.008909Z","shell.execute_reply.started":"2022-04-25T04:45:04.996352Z","shell.execute_reply":"2022-04-25T04:45:05.007781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a function to get scores\ndef get_score(model, scaler, alpha):\n    my_pipeline=Pipeline(steps=[('My scaler', scaler()), ('My classifier',model(alpha =alpha))])\n    scores=-1 * cross_val_score(my_pipeline, X_train, y_train, cv=3, scoring='neg_mean_squared_error')\n    return scores.mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:45:07.153328Z","iopub.execute_input":"2022-04-25T04:45:07.153727Z","iopub.status.idle":"2022-04-25T04:45:07.160725Z","shell.execute_reply.started":"2022-04-25T04:45:07.153694Z","shell.execute_reply":"2022-04-25T04:45:07.15954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a function so I can easily find the best alpha with different linear models & scalers and compare model performances later\ndef model_scores(model, scaler, alphas):\n    results={}\n    best_score=float('inf')\n    best_alpha=0\n    for alpha in alphas:\n        score= get_score(model=model, scaler=scaler, alpha=alpha)\n        if score < best_score:\n            best_score=score\n            best_alpha=alpha\n        results[alpha]=score\n    \n    print(f\"\\nBest alpha: {best_alpha} with score of {best_score}\")\n    fig, ax = plt.subplots(figsize=(10,5))\n    ax.plot(list(results.keys()), list(results.values()), markersize=5, marker=\"o\", color=\"royalblue\")\n    ax.set_title(str(model) + str(scaler))\n    ax.set_xlabel(\"alpha\")\n    ax.set_ylabel(\"MSE score\")   \n    return best_score\n","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:45:12.235027Z","iopub.execute_input":"2022-04-25T04:45:12.235745Z","iopub.status.idle":"2022-04-25T04:45:12.245697Z","shell.execute_reply.started":"2022-04-25T04:45:12.235707Z","shell.execute_reply":"2022-04-25T04:45:12.244836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_results_lasso(model, scaler, alpha):\n    X_cv, X_test_train, y_cv, y_test_train= train_test_split(X_train, y_train, test_size=0.2, random_state=123)\n    my_pipeline=Pipeline(steps=[('My scaler', scaler()), ('My classifier',model(alpha =alpha))])\n    my_pipeline.fit(X_cv,y_cv)\n    y_pred=my_pipeline.predict(X_test_train) \n    plt.figure(figsize=(10,10))\n    plt.scatter(y_test_train, y_pred, c='royalblue')\n    plt.xlabel(\"log(Actual Sale Price) $ \")\n    plt.ylabel(\"log(Predicted Sale Price) $ \")\n    plt.title(str(model)+str(scaler))\n    plt.grid()\n    plt.show() \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:45:40.322715Z","iopub.execute_input":"2022-04-25T04:45:40.323206Z","iopub.status.idle":"2022-04-25T04:45:40.331702Z","shell.execute_reply.started":"2022-04-25T04:45:40.323174Z","shell.execute_reply":"2022-04-25T04:45:40.330401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a score dictionary to later compare scores of different models\nscores={}","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:45:42.883299Z","iopub.execute_input":"2022-04-25T04:45:42.884099Z","iopub.status.idle":"2022-04-25T04:45:42.887995Z","shell.execute_reply.started":"2022-04-25T04:45:42.884055Z","shell.execute_reply":"2022-04-25T04:45:42.886935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lasso with StandardScaler\nscores[\"LS_Std\"]=model_scores(Lasso, StandardScaler, alphas=np.logspace (-3,-2, num=10))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:56:42.011832Z","iopub.execute_input":"2022-04-25T04:56:42.012559Z","iopub.status.idle":"2022-04-25T04:56:44.259865Z","shell.execute_reply.started":"2022-04-25T04:56:42.012506Z","shell.execute_reply":"2022-04-25T04:56:44.258539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take a look to see if we have additional columns that created through feature pre-processing\na=set(list(X_train.columns))\nb=set(list(X_test.columns))\nc=b.difference(a)\nc","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:45:54.399099Z","iopub.execute_input":"2022-04-25T04:45:54.399579Z","iopub.status.idle":"2022-04-25T04:45:54.407606Z","shell.execute_reply.started":"2022-04-25T04:45:54.399544Z","shell.execute_reply":"2022-04-25T04:45:54.406626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#It seems test set has addtional columns because some testing set columns has missing values while the same columns in training set there are no missing values\n#I will just drop those additional column\nX_test=X_test[X_train.columns]\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:49:17.864131Z","iopub.execute_input":"2022-04-25T04:49:17.8648Z","iopub.status.idle":"2022-04-25T04:49:17.874258Z","shell.execute_reply.started":"2022-04-25T04:49:17.86473Z","shell.execute_reply":"2022-04-25T04:49:17.872886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#draw actual vs. predicted values - Lasso with Standard Scaler\ndraw_results_lasso(Lasso, StandardScaler, alpha= 0.0027825594022071257)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:56:00.652338Z","iopub.execute_input":"2022-04-24T23:56:00.65283Z","iopub.status.idle":"2022-04-24T23:56:00.953112Z","shell.execute_reply.started":"2022-04-24T23:56:00.652791Z","shell.execute_reply":"2022-04-24T23:56:00.952165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lasso with MinMaxScaler\nscores[\"LS_MM\"]=model_scores(Lasso, MinMaxScaler, alphas=np.logspace (-3.6,-3.2, num=12))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-25T04:56:54.83798Z","iopub.execute_input":"2022-04-25T04:56:54.838429Z","iopub.status.idle":"2022-04-25T04:56:57.318751Z","shell.execute_reply.started":"2022-04-25T04:56:54.838392Z","shell.execute_reply":"2022-04-25T04:56:57.317434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#draw actual vs. predicted values - Lasso with MinMax Scaler\ndraw_results_lasso(Lasso,MinMaxScaler, alpha= 0.000296980047740645)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:56:08.813104Z","iopub.execute_input":"2022-04-24T23:56:08.81366Z","iopub.status.idle":"2022-04-24T23:56:09.15415Z","shell.execute_reply.started":"2022-04-24T23:56:08.813626Z","shell.execute_reply":"2022-04-24T23:56:09.152302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ridge with StandardScaler\nscores[\"Rig_Std\"]=model_scores(Ridge, StandardScaler, alphas=np.logspace (1.7,1.9, num=10))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:56:58.724168Z","iopub.execute_input":"2022-04-25T04:56:58.724639Z","iopub.status.idle":"2022-04-25T04:56:59.90231Z","shell.execute_reply.started":"2022-04-25T04:56:58.724603Z","shell.execute_reply":"2022-04-25T04:56:59.901108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#draw actual vs. predicted values - Ridge with Standard Scaler\ndraw_results_lasso(Ridge,StandardScaler, alpha= 58.434141337351754)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:57:06.903565Z","iopub.execute_input":"2022-04-25T04:57:06.903968Z","iopub.status.idle":"2022-04-25T04:57:07.202796Z","shell.execute_reply.started":"2022-04-25T04:57:06.903937Z","shell.execute_reply":"2022-04-25T04:57:07.201913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ridge with MinMaxScaler\nscores[\"Rid_MM\"]=model_scores(Ridge, MinMaxScaler, alphas=np.logspace (-1.4,0.5, num=10))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:57:10.597188Z","iopub.execute_input":"2022-04-25T04:57:10.597884Z","iopub.status.idle":"2022-04-25T04:57:11.644247Z","shell.execute_reply.started":"2022-04-25T04:57:10.597835Z","shell.execute_reply":"2022-04-25T04:57:11.643238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#draw actual vs. predicted values - Ridge with MinMax Scaler\ndraw_results_lasso(Ridge, MinMaxScaler, alpha= 58.434141337351754)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:57:14.951989Z","iopub.execute_input":"2022-04-25T04:57:14.952624Z","iopub.status.idle":"2022-04-25T04:57:15.245041Z","shell.execute_reply.started":"2022-04-25T04:57:14.952583Z","shell.execute_reply":"2022-04-25T04:57:15.243739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tree models","metadata":{}},{"cell_type":"markdown","source":"#### Decision Tree","metadata":{}},{"cell_type":"code","source":"#First let's take a look at a simple decision tree. I only used max_depth of 3 so I can take visualize it clearly here\n#from sklearn.tree import DecisionTreeRegressor\ndt_regr = DecisionTreeRegressor(max_depth=3, random_state=1234)\nmodel = dt_regr.fit(X_train, y_train)\n#text_representation = tree.export_text(dt_regr)\n#print(text_representation)\nfig = plt.figure(figsize=(20,5))\n_ = tree.plot_tree(dt_regr, feature_names=X_train.columns, filled=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T23:56:34.007838Z","iopub.execute_input":"2022-04-24T23:56:34.008254Z","iopub.status.idle":"2022-04-24T23:56:35.091263Z","shell.execute_reply.started":"2022-04-24T23:56:34.008222Z","shell.execute_reply":"2022-04-24T23:56:35.08986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest","metadata":{}},{"cell_type":"code","source":"#Random Forest\nX_cv, X_test_train, y_cv, y_test_train= train_test_split(X_train, y_train, test_size=0.2, random_state=123)\nrf=RandomForestRegressor()\nrf.fit(X_cv, y_cv)\ny_pred=rf.predict(X_test_train)\nrmse_test = mean_squared_error(y_test_train, y_pred)\nprint('Test set RMSE of rf: {:.2f}'.format(rmse_test))\nscores[\"RF\"]=rmse_test\n\n#Let's see feature importance of Random Forest\ncoef = pd.Series(data=rf.feature_importances_, index = X_train.columns)\nimp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\nmatplotlib.rcParams['figure.figsize'] = (10, 12)\nimp_coef.plot(kind = \"barh\", color=\"violet\")\nplt.title(\"Feautre Importances in the Random Forest Model\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:58:18.004534Z","iopub.execute_input":"2022-04-25T04:58:18.005353Z","iopub.status.idle":"2022-04-25T04:58:20.531761Z","shell.execute_reply.started":"2022-04-25T04:58:18.005308Z","shell.execute_reply":"2022-04-25T04:58:20.530534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gradient Boosting","metadata":{}},{"cell_type":"code","source":"#Use Random Search to tune Gradient Boost\nnum_leaves=list(range(4,6))\nlearn_rate_list=[0.003,0.005,0.01]\nn_estimators_list=[5000,6000,7000]\nmax_bin=[150,200,250]\nbagging_fraction=np.linspace(0.7,0.8,5)\nbagging_freq=[4,5,6]\nbagging_seed=[6,7,8]\nfeature_fraction=np.linspace(0.15,0.25,5)\nfeature_seed=[6,7,8]\n\nparams_grid={\"num_leaves\":num_leaves,\n        \"learning_rate\":learn_rate_list,\n        \"n_estimators\":  n_estimators_list,\n        \"max_bin\": max_bin,\n        \"bagging_fraction\":bagging_fraction,\n        \"bagging_freq\":bagging_freq,\n        \"bagging_seed\":bagging_seed,\n        \"feature_fraction\":feature_fraction,\n        \"feature_fraction_seed\":feature_seed  }\nrandom_GBM_class=RandomizedSearchCV(estimator = LGBMRegressor(objective='regression',                                   \n                                       \n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1),\n                              param_distributions=params_grid,\n                              n_iter=40, \n                              scoring='neg_mean_squared_error',\n                              cv=2,\n                              refit=True, \n                              return_train_score=True )\nrandom_GBM_class.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:08:47.116848Z","iopub.execute_input":"2022-04-25T04:08:47.117209Z","iopub.status.idle":"2022-04-25T04:12:18.838689Z","shell.execute_reply.started":"2022-04-25T04:08:47.11718Z","shell.execute_reply":"2022-04-25T04:12:18.837653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring Gradient Boost results\ncv_results_df = pd.DataFrame(random_GBM_class.cv_results_).sort_values(by=\"rank_test_score\")\ncv_results_df.head(15)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:16:39.814143Z","iopub.execute_input":"2022-04-25T04:16:39.81462Z","iopub.status.idle":"2022-04-25T04:16:39.876671Z","shell.execute_reply.started":"2022-04-25T04:16:39.814584Z","shell.execute_reply":"2022-04-25T04:16:39.875281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Take a look at the best score and the best parameters\nbest_score = -1*random_GBM_class.best_score_\nscores[\"GB\"] =best_score\nprint(f\"best score is {best_score}\")\nprint(f\"best parameters are {random_GBM_class.best_params_}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:58:56.298661Z","iopub.execute_input":"2022-04-25T04:58:56.299431Z","iopub.status.idle":"2022-04-25T04:58:56.309347Z","shell.execute_reply.started":"2022-04-25T04:58:56.299389Z","shell.execute_reply":"2022-04-25T04:58:56.307621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's see feature importance of Gradient Boost\ncoef = pd.Series(data=random_GBM_class.best_estimator_.feature_importances_, index = X_train.columns)\nimp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\nmatplotlib.rcParams['figure.figsize'] = (10, 12)\nimp_coef.plot(kind = \"barh\", color=\"violet\")\nplt.title(\"Feautre Importances in the Gradient Boost Model\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:17:45.91688Z","iopub.execute_input":"2022-04-25T04:17:45.91755Z","iopub.status.idle":"2022-04-25T04:17:46.255434Z","shell.execute_reply.started":"2022-04-25T04:17:45.917511Z","shell.execute_reply":"2022-04-25T04:17:46.254058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost","metadata":{}},{"cell_type":"code","source":"#Use Random Search to tune XGBoost\nmax_depth_list=[4,5]\nlearn_rate_list=[0.005,0.01]\nn_estimators_list=[6000,7000,8000]\ncolsample_bytree_list=[0.9]\nmin_child_weight_list=[0,1]\nsubsample_list=[0.9,1]\nalpha_list= [0,1,2]\nlambda_list =[7,8]\nreg_alpha_list=[0.00005,0.00006,0.00007]\ngamma_list=[0.5,0.6,0.7]\n\nparams_grid={\"max_depth\":max_depth_list,\n        \"learning_rate\":learn_rate_list,\n        \"n_estimators\":  n_estimators_list,\n        \"colsample_bytree\":colsample_bytree_list,\n        \"min_child_weight\": min_child_weight_list,\n        \"subsample\":subsample_list,\n        \"alpha\":alpha_list,\n        \"lambda\":lambda_list,\n      \"reg_alpha\": reg_alpha_list,\n        \"gamma\":gamma_list }\nrandom_XGBst_class=RandomizedSearchCV(estimator = XGBRegressor(\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       random_state=42),\n                              param_distributions=params_grid,\n                              n_iter=20, \n                              scoring='neg_mean_squared_error',\n                              cv=2,\n                              refit=True, \n                              return_train_score=True )\nrandom_XGBst_class.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:21:12.033351Z","iopub.execute_input":"2022-04-25T04:21:12.033735Z","iopub.status.idle":"2022-04-25T04:30:51.3038Z","shell.execute_reply.started":"2022-04-25T04:21:12.033703Z","shell.execute_reply":"2022-04-25T04:30:51.301975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring Gradient Boost results\ncv_results_df = pd.DataFrame(random_XGBst_class.cv_results_).sort_values(by=\"rank_test_score\")\ncv_results_df.head(7)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:53:39.226667Z","iopub.execute_input":"2022-04-25T04:53:39.227119Z","iopub.status.idle":"2022-04-25T04:53:39.270896Z","shell.execute_reply.started":"2022-04-25T04:53:39.227083Z","shell.execute_reply":"2022-04-25T04:53:39.269911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Take a look at the best score and the best parameters\nbest_score = -1*random_XGBst_class.best_score_\nscores['XGB']=best_score\nprint(f\"best score is {best_score}\")\nprint(f\"best parameters are {random_XGBst_class.best_params_}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T04:53:42.13863Z","iopub.execute_input":"2022-04-25T04:53:42.139432Z","iopub.status.idle":"2022-04-25T04:53:42.145847Z","shell.execute_reply.started":"2022-04-25T04:53:42.139367Z","shell.execute_reply":"2022-04-25T04:53:42.145004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's see feature importance XGoost\ncoef = pd.Series(data=random_XGBst_class.best_estimator_.feature_importances_, index = X_train.columns)\nimp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\nmatplotlib.rcParams['figure.figsize'] = (10, 12)\nimp_coef.plot(kind = \"barh\", color=\"violet\")\nplt.title(\"Feautre Importances in the XGBoost Model\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T03:37:24.584773Z","iopub.execute_input":"2022-04-25T03:37:24.58517Z","iopub.status.idle":"2022-04-25T03:37:25.02875Z","shell.execute_reply.started":"2022-04-25T03:37:24.585137Z","shell.execute_reply":"2022-04-25T03:37:25.027759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores","metadata":{"execution":{"iopub.status.busy":"2022-04-25T05:03:37.949698Z","iopub.execute_input":"2022-04-25T05:03:37.950292Z","iopub.status.idle":"2022-04-25T05:03:37.959317Z","shell.execute_reply.started":"2022-04-25T05:03:37.950241Z","shell.execute_reply":"2022-04-25T05:03:37.957975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict using different models\nX_cv, X_test_train, y_cv, y_test_train= train_test_split(X_train, y_train, test_size=0.2, random_state=123)\n\n#Lasso with Standard Scaler\nmy_pipeline=Pipeline(steps=[('My scaler', StandardScaler()), ('My classifier',Lasso(alpha =0.0027825594022071257))])\nmy_pipeline.fit(X_train,y_train)\ny_pred_LS=my_pipeline.predict(X_test)\ny_pred_LS_train=my_pipeline.predict(X_test_train)\n\n#Random Forest\ny_pred_RF=rf.predict(X_test)\ny_pred_RF_train=rf.predict(X_test_train)\n\n#Gradient Boost\ny_pred_GB=random_GBM_class.predict(X_test)\ny_pred_GB_train=random_GBM_class.predict(X_test_train)\n\n#XGBoost\ny_pred_XGB=random_XGBst_class.predict(X_test)\ny_pred_XGB_train=random_XGBst_class.predict(X_test_train)\n\ny_pred_train=0.3*y_pred_LS_train+0.1*y_pred_RF_train+0.5*y_pred_GB_train+0.1*y_pred_GB_train\ny_pred =0.3*y_pred_LS+0.1*y_pred_RF+0.5*y_pred_GB+0.1*y_pred_GB\n\nrmse_test = mean_squared_error(y_test_train, y_pred_train)\nprint(rmse_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-25T05:26:28.555734Z","iopub.execute_input":"2022-04-25T05:26:28.556184Z","iopub.status.idle":"2022-04-25T05:26:28.936253Z","shell.execute_reply.started":"2022-04-25T05:26:28.556148Z","shell.execute_reply":"2022-04-25T05:26:28.935313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 5: Get Results on Testing Data","metadata":{}},{"cell_type":"code","source":"#Use the best model to predict testing data dependent variable \"SalePrice\"\ny_test_pred=np.exp(y_pred) ","metadata":{"execution":{"iopub.status.busy":"2022-04-25T05:26:31.540438Z","iopub.execute_input":"2022-04-25T05:26:31.541225Z","iopub.status.idle":"2022-04-25T05:26:31.545766Z","shell.execute_reply.started":"2022-04-25T05:26:31.541182Z","shell.execute_reply":"2022-04-25T05:26:31.544853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save prediction results \nmy_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': y_test_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission4.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T05:26:56.716643Z","iopub.execute_input":"2022-04-25T05:26:56.717109Z","iopub.status.idle":"2022-04-25T05:26:56.732766Z","shell.execute_reply.started":"2022-04-25T05:26:56.717074Z","shell.execute_reply":"2022-04-25T05:26:56.731709Z"},"trusted":true},"execution_count":null,"outputs":[]}]}